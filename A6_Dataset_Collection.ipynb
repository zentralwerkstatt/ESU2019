{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2019 Fabian Offert, [MIT License](LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using the `BeautifulSoup`library to find specific tags on websites and the `requests` library to download, i.e. \"request\" websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import sys\n",
    "sys.path.append('lib/')\n",
    "from esu_util import show_img, np_PIL, PIL_np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our toy dataset: WGA-small (2200 samples)\n",
    "\n",
    "This is a (for ML) tiny dataset, scraped from the web gallery of art and consisting of 2x1100 high-quality images in two classes: \"portrait\" and \"landscape\" paintings. 1000 images of each classes are reserved for *training*, 100 images of each class are reserverd for *testing* our machine learning classifier. It is available as part of the workshop repository in the `wga`folder. The below code presents some randomly picked samples from the dataset, utilizing coding practices we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'wga' # Relative path\n",
    "imgs = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            imgs.append(f'{root}/{file}')\n",
    "print(f'{len(imgs)} files found')\n",
    "random_imgs = np.random.choice(imgs, 3)\n",
    "\n",
    "for img in random_imgs:\n",
    "    show_img(Image.open(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our real-life dataset: MoMA collection (64780 samples)\n",
    "\n",
    "The New York City Museum of Modern Art collection consist of almost 200,000 works, 81,000 of which are available online. 64780 of these avaible works are part of this (\"unsupervised\", i.e. metadata-less), high-quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/fabian/datasets-live/moma/images' # Relative path\n",
    "imgs = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            imgs.append(f'{root}/{file}')\n",
    "print(f'{len(imgs)} files found')\n",
    "random_imgs = np.random.choice(imgs, 3)\n",
    "\n",
    "for img in random_imgs:\n",
    "    show_img(Image.open(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screenshot of the source code in browser with relevant lines highlighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A questionable mega-dataset: ImageNet\n",
    "\n",
    "- What is it: http://www.image-net.org/\n",
    "- Download the ILSVRC2012 subset: http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a dataset: MoMA example \n",
    "\n",
    "Some datasets are harder to scrape then others. The MoMA website is a particular easy example. Generally, the process is always the same: inspect the URL and source code of the website with regard to how it presents a single work/image file. Then automate the this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/moma-back.jpg)\n",
    "\n",
    "![](img/moma-front.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a directory to save the downloaded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = 'moma'\n",
    "TOTAL_PAGES = 1000000 # Unclear what the limit is, getting 404s/no images is \"cheap\" enough though to brute-force\n",
    "if not os.path.exists(DIRECTORY): \n",
    "    os.makedirs(DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to save an image file from a direct image URL, specific to the MoMA website. This version also has the option to save some metadata into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv   \n",
    "\n",
    "def save(url, meta=None, meta_fn=None):\n",
    "    data = requests.get(url).content\n",
    "    name = url.split('?sha=')[-1] # SHA as name\n",
    "    file = f'{DIRECTORY}/{name}.jpg'\n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(data)\n",
    "        \n",
    "    if meta:\n",
    "        meta.append(file) # Also write the name of the local file\n",
    "        with open(meta_fn, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to process one page on the MoMA website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(page):\n",
    "    url = f'https://www.moma.org/collection/works/{page}'  \n",
    "        \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: # If we get a positive response from the server...\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # Parse the page\n",
    "\n",
    "        # Find the image on the page\n",
    "        imgs = soup.findAll('img', 'picture__img--focusable') # Find a specific class of the img tag\n",
    "        \n",
    "        # Find the metadata on the page\n",
    "        # We know that it is the second 'meta' tag with the name 'stitle' that we want\n",
    "        # We also know the format is 'author. work. year.' so we can split by '. '\n",
    "        meta = soup.findAll('meta', {'name':'stitle'})\n",
    "        content = meta[1].get('content').split('. ')\n",
    "        \n",
    "        if len(content) > 3:\n",
    "            content = [content[0], content[1], content[2]+'. '+content[3]]\n",
    "              \n",
    "        if imgs:\n",
    "            src = imgs[0].get('src') # Get the URL of the first found image\n",
    "            save(f'https://www.moma.org{src}', meta=content, meta_fn='moma_meta.csv') # Save the image\n",
    "            return True # Only return true if image was downloaded\n",
    "        else:\n",
    "            print(f'No image links on page {page}')\n",
    "            return False\n",
    "    else: \n",
    "        print(f'Response {response.status_code} for page {page}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every work has a unique page number, starting (for some reason) with 200000 - found by trial and error\n",
    "n = 0\n",
    "for page in tnrange(200000, TOTAL_PAGES):\n",
    "    n += process_page(page) # Keep track of nr. of downloaded images\n",
    "    if page % 20 == 0: # Print status every 20 pages\n",
    "        print(f'{n} images downloaded so far...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this process can be sped up massively by using multiple threads to download the images. An implementation is provided as a [Python script here](moma-scraper.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercises</font>\n",
    "\n",
    "1. Find an online image dataset that interests you to scrape - it does not have to be gigantic, nor does it have to be high-quality. Start by looking at the page source to see if the scraping could be automated. Look for possible tags to find images. Finally, try to adapt the script above for your dataset.\n",
    "2. How could we add some metadata to this process?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
